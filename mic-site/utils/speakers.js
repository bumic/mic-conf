module.exports = {
	speakersJSON: {

		kenneth_stanley: {
			name: 'Kenneth Stanley',
			bio: 'Kenneth O. Stanley is Charles Millican Professor of Computer Science at the University of Central Florida and director there of the Evolutionary Complexity Research Group. He was also a co-founder of Geometric Intelligence Inc., which was acquired by Uber to create Uber AI Labs, where he is now also a senior engineering manager and staff scientist.  He received a B.S.E. from the University of Pennsylvania in 1997 and received a Ph.D. in 2004 from the University of Texas at Austin.  He is an inventor of the Neuroevolution of Augmenting Topologies (NEAT), HyperNEAT, and novelty search neuroevolution algorithms for evolving complex artificial neural networks.  His main research contributions are in neuroevolution (i.e. evolving neural networks), generative and developmental systems, coevolution, machine learning for video games, interactive evolution, and open-ended evolution. He has won best paper awards for his work on NEAT, NERO, NEAT Drummer, FSMC, HyperNEAT, novelty search, and Galactic Arms Race. His original 2002 paper on NEAT also received the 2017 ISAL Award for Outstanding Paper of the Decade 2002 - 2012 from the International Society for Artificial Life.    He is a coauthor of the popular science book, "Why Greatness Cannot Be Planned: The Myth of the Objective" (published by Springer), and has spoken widely on its subject.',
			id: '',
		},

		leslie_kaelbling: {
			name: 'Leslie Kaelbling',
			bio: 'Leslie is a Professor at MIT. She has an undergraduate degree in Philosophy and a PhD in Computer Science from Stanford, and was previously on the faculty at Brown University. She was the founding editor-in-chief of the Journal of Machine Learning Research. Her research agenda is to make intelligent robots using methods including estimation, learning, planning, and reasoning. She is not a robot.',
			id: '',
		},

		matt_scott: {
			name: 'Matt Scott',
			bio: 'Matt Scott is Co-Founder and CTO of Malong Technologies, an award-winning artificial intelligence startup based in China. The company produces leading computer vision technologies for the retail and medical industries. Matt has 15+ years R&D experience in computer vision and machine learning. </br> He is a Senior Member of the IEEE (SMIEEE), has published 70+ patents, and over a dozen research papers in top scientific conferences and journals, including cover featured articles in the Proceedings of the IEEE and IEEE Computer. At CVPR 2017, Matt and his team won first place in the WebVision Challenge, a worldwide computer vision contest from Google. </br> In 2014, Matt co-founded Malong Technologies with CEO Dinglong Huang. The company now has 150+ employees and recently completed a B-Round of venture capital funding led by Softbank in China. Accenture also chose Malong as its first investment in China, as part of a strategic alliance announced in July 2018. Matt leads the R&D efforts of the company creating state-of-the-art computer vision technology and products. Malong was selected as a 2018 Technology Pioneer by the World Economic Forum. </br> Prior to Malong, he was at Microsoft for 10 years, working as a senior research development lead at Microsoft Research on computer vision, machine learning, and NLP. In 2018, Matt was recognized by Fast Company as one of the 100 Most Creative People in Business in China. His work has been featured in The Wall Street Journal, Forbes, The New York Times, the Financial Times, PBS Nightly News, CNBC, and The Discovery Channel, among other media. </br> Matt is a proud alumnus of Boston University',
		},

		rachel_manzelli: {
			name: 'Rachel Manzelli',
			bio: '',
		},

		diane_feddema: {
			name: 'Diane Feddema',
			bio: 'Diane Feddema is a principal engineer at Red Hat Inc Canada, in the AI Center Of Excellence.  Diane is currently focused on developing and applying big data techniques for performance analysis, automating these analyses and displaying data in novel ways.  Previously Diane was a performance engineer at the National Center for Atmospheric Research, NCAR, working on optimization and tuning in parallel global climate models.',
		},

		adji_buosso_dieng: {
			name: 'Adji Buosso-Dieng',
			bio: "Adji is a Ph.D candidate at Columbia University where she is jointly advised by David Blei and John Paisley. Her work at Columbia is about combining probabilistic graphical modeling and deep learning to design better sequence models. As a PhD student she has shared her time between Columbia and industrial labs such as Microsoft Research, Facebook AI Research, and DeepMind. Her hope is that her research can be applied to many real world applications particularly to natural language understanding. </br> Prior to joining Columbia Adji worked as a Junior Professional Associate at the World Bank. She did her undergraduate training in France where she attended Lycee Henri IV and Telecom ParisTech--France's Grandes Ecoles system. She holds a Diplome d'Ingenieur from Telecom ParisTech and spent the third year of Telecom ParisTech's curriculum at Cornell University where she earned a Master in Statistics.",
		},

		kate_saenko: {
			name: 'Kate Saenko',
			bio: 'Kate is an Associate Professor of Computer Science at Boston University and director of the <a href="http://ai.bu.edu/">Computer Vision and Learning Group</a>. Her research interests are in the broad area of Artificial Intelligence with a focus on Adaptive Machine Learning, Learning for Vision and Language Understanding, and Deep Learning.',
		},

		kathy_pham: {
			name: 'Kathy Pham',
			bio: 'Kathy Pham is a computer scientist and product leader who currently is a senior fellow and adjunct lecturer at the Kennedy School of Government, Fellow at the Berkman Klein Center where she co-leads the Ethical Tech Working Group, and Fellow in Residence at the Mozilla Foundation where she leads a project on ethics in Computer Science curricula. Kathy formerly as at Google, IBM, and the United States Digital Service at the White House where she was a founding product and engineering member. She holds a Bachelors and Masters in Computer Science from Georgia Tech and Supelec. ',
		},

		millie_liu: {
			name: 'Millie Liu',
			bio: 'Millie Liu has focused her career on helping entrepreneurs with deep technology turn their ideas into great businesses with global reach. </br> She was previously at APT, an enterprise data analytics startup acquired by Mastercard for $600m where she helped Fortune 50 clients such as Walmart and P&G make better strategic decisions leveraging data. She was also the co-founder of an MIT startup working on unsupervised event detection, which later pivoted and became Infervision, an AI precision healthcare platform backed by Sequoia China. Millie is on the advisory board of MIT CSAIL(Computer Science and Artificial Intelligence Lab). She holds a Master of Finance degree from MIT and B.S. in Mathematics from the University of Toronto.',
		},

		aidan_gomez: {
			name: 'Aidan Gomez',
			bio: "Aidan is a doctoral student of Yarin Gal and Yee Whye Teh at The University of Oxford. He leads a research group, called FOR.ai, focussing on providing resources, mentorship, and facilitating collaboration between academia and industry. Aidan's research deals in understanding and improving neural networks and their applications. Previously, he worked with Geoffrey Hinton and Łukasz Kaiser on the Google Brain team.",
			title: 'Targeted Dropout and Bitrot: Simple and Effective Techniques for Sparsification and Quantization',
			abstract: 'Targeted Dropout (TD) is a simple, easily implemented technique that can drastically improve the post hoc sparsification of neural networks. This talk will present TD and discuss some recent results in sparsification and quantization that suggest neural network over-parameterization presents great opportunity for both faster training and inference.',
		},

		amy_wang: {
			name: 'Amy Wang',
			bio: 'Amy Wang is a third-year student at Brown University concentrating in Computational Biology. Her interests within the field include data visualization and machine learning.',
		},

		andrew_rouditchenko: {
			name: 'Andrew Rouditchenko',
			bio: 'Andrew Rouditchenko is a senior at MIT majoring in Electrical Engineering and Computer Science, with a focus on machine learning and audio processing. Andrew has conducted research with MIT CSAIL’s Computer Vision Research Group, Intel’s Artificial Intelligence Research Group, and MIT BCS’s Computational Audition Research Group. Andrew is interested in advancing computational techniques for understanding perception and intelligence.',
			title: 'The Sound of Pixels',
			abstract: 'We introduce PixelPlayer, a deep learning system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into aset of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources. More information is available at <a href="http://sound-of-pixels.csail.mit.edu/">http://sound-of-pixels.csail.mit.edu/</a>.',
		},

		ari_brown: {
			name: 'Ari Brown',
			bio: 'Ari Brown is in the dual program between Tufts University and New England Conservatory of Music, where he studies computer science and music composition.  Following his combined passions, Ari co-founded Cherrystems Music in 2016, which aims to make music production as easy as choosing sounds and a style using artificial intelligence. Ari also recently contributed software to the Mars 2020 rover mission at NASA’s Jet Propulsion Laboratory. ',
			title: 'The Semantic-Hypnotic Model of Music Perception',
			abstract: 'Music theorists and composers commonly analyze and synthesize music based on rules established in the Classical period, such as harmonic progressions and tonal language.  However, popular music today (sometimes unknowingly) employs rules established in the sixteenth century, much before the Classical period, which are based on the perception of music by the audience rather than how it is constructed.  The rules of sixteenth century counterpoint, in combination with software, may help bridge the gap between the music creation process and novice music enthusiasts.  I propose the Semantic-Hypnotic model of music perception, which lays the groundwork for more greatly enhanced human-computer interaction in the context of music creation.  I show an abstract implementation of the model in use, and discuss future implications that could more broadly apply to hit song science, automated mixing, and research in music cognition',
		},

		ruo_yu_david_tao: {
			name: 'Ruo Yu (David) Tao',
			bio: 'David Tao is a Singaporean undergraduate CS student currently studying at McGill University, and also a research intern at Microsoft Research Montreal. His current research resides in the intersection between language and reinforcement learning.',
			title: 'Learning with text-based games',
			abstract: 'The exponential nature of action and state spaces make text-based dialogue systems difficult to solve for most machine algorithms. This is especially true for dynamic action spaces that condition on state. To solve this problem we propose an architecture to learn our action space and a control policy to solve text-based games - a simplistic model for dialogue systems. We use games generated by the TextWorld framework and show promising results on a carrot-cooking game.',
		},

		giulia_pintea: {
			name: 'Giulia Pintea',
			bio: 'Giulia Pintea is a senior at Simmons University double-majoring in Biostatistics and Psychology. Over this past summer, she participated in the Research in Industrial Projects for Students (RIPS) program at University of California, Los Angeles, where her and her team worked on a project that focused on optimizing the design of  radiation therapy in prostate cancer patients through machine learning. Giulia and her team members hope that this research will inspire others to employ machine learning methods in the biomedical field.',
			title: 'Using Quality-of-Life Scores to Guide Prostate Radiation Therapy Dosing',
			abstract: 'Prostate cancer has one of the highest patient survival rates for cancer. The primary focus of treatment has moved to how to treat without decreasing the quality-of-life of patients. This project seeks to understand the surprisingly not-well-known connection between the radiation a patient receives and the symptoms a patient experiences. We use deep learning methods on small datasets as well as statistical analysis methods to evaluate organ sensitivity. In the biomedical field, data is expensive and not abundant, which leads to small datasets. Transfer learning is one method for handling small datasets, but with specialized networks we can lose its full advantages. We use image flipping and curvature-based interpolation methods to create more data in order to leverage transfer learning. Using interpolated and augmented data, we can train a convolutional autoencoder network to get near-optimal starting points for the weights in our convolutional neural network for analyzing the relationship between patient-reported quality-of-life and radiation. Furthermore, we use analysis of variance and logistic regression to analyze organ sensitivity to radiation and develop dosage thresholds for each organ region. We identify regions of both the bladder and rectum that are highly correlated with changes in individual symptoms. Finally, we estimate radiation therapy dosage thresholds to determine how high radiation therapy dosage needs to be in order to trigger collateral symptoms. Connecting deep learning methods and organ sensitivity provides a framework to inform patient care in the context of their quality-of-life.',
		},

		guy_aridor: {
			name: 'Guy Aridor',
			bio: 'Guy Aridor is a PhD student in Economics at Columbia University whose research primarily focuses on using tools from theoretical computer science to help understand problems in behavioral economics and industrial organization. Previously, he worked as a software engineer at Nutonian and HubSpot as well as studied computer science, mathematics, and economics at Boston University.',
			title: 'What can artificial intelligence teach economics?',
			abstract: 'While the fields of cognitive science, computational neuroscience, and artificial intelligence have progressively built off each other in the quest of reverse engineering intelligence, economics has largely remained agnostic to this progress. Historically economics was only concerned with observing the choices individuals made and not with the process decision makers utilized in coming to these choices. However, in recent years behavioral economics and neuroeconomics have emerged with the purpose of focusing on models that take into account the decision making process in order to better rationalize economic behavior that previously had been at odds with classic economic decision theory. </br> In this talk we discuss how the direction that behavioral economics and neuroeconomics is taking is similar to the direction taken in parts of the artificial intelligence literature towards striving for designing bounded optimal agents instead of perfectly rational agents. We discuss what insights from this literature can inform the models utilized in economics and, conversely, how the models in economics can inform the artificial intelligence research community. Finally, we present preliminary experimental evidence from a human problem solving experiment as an application of the applicability of the concepts that are discussed.',
		},

		jessica_edwards: {
			name: 'Jessica Edwards',
			bio: "Jessica Edwards is a sophomore at Harvard College concentrating in Computer Science and intending to get a secondary in Educational Studies. This past summer she was a Research Assistant at Johns Hopkins University's Human Language Technology Center of Excellence and is currently participating in computer science research term-time through the Learning, Innovation, and Technology Lab in Harvard's Graduate School of Education. At Harvard, she is also involved in Harvard's Women in Computer Science organization, volunteers at Y2Y Harvard Square Homeless Shelter, and is a member of the Kuumba Singers of Harvard College.",
			title: 'Multilingual Text Search in Image Collection',
			abstract: 'This talk will focus on computer vision techniques used to explore core aspects in the optical character recognition (OCR) pipeline, specifically focusing on text localization. I will explain how the team I worked with helped make the Efficient and Accurate Scene Text detector (EAST) more accurate by running models on more data, focusing on how synthetic data, real data augmentation, and generative adversarial networks (GANs) helped in this process. I will also talk about my experience doing research this summer in the Summer Camp for Applied Language Exploration (SCALE) program as well as attending workshops through JHU’s Center for Language and Speech Processing (CLSP).',
		},

		jessica_yu: {
			name: 'Jessica Yu',
			bio: 'Jessica is an MIT senior studying computer science, with a focus in product-driven machine learning and backend development. Her interests outside work include history, vocals, and analyzing film.',
			title: 'VideoQA: A Convolutional Approach to Multimodal Video Retrieval',
			abstract: "Recent years have seen an accelerating trend from traditional classroom environments to online video lectures for technical certifications and higher level education. These video resources, including massive open online courses (MOOCs) such as MIT OpenCourseWare, lead the way toward democratizing access to education and useful skills. However, without a human mentor or teacher for guidance, students struggle to navigate large monolithic lectures if they seek the answer to a specific question. VideoQA is a step toward empowering MOOC students in this capacity using state-of-the-art machine learning. In particular, we formulate, implement, and tune a convolutional neural network that takes as input a user's question and a large video, and outputs the section of the video most likely to answer the given question. Along the way, we learn textual and visual representations, and synthesize them to find semantic similarities between a question and each video segment. Hence, our model tackles the challenging problem of learning to compare and contrast multimodal inputs such that it can accurately rank portions of an educational video by their relevance to the user's specific need.",
		},

		mary_dong: {
			name: 'Mary Dong',
			bio: "Mary Dong is a sophomore at Brown University concentrating in Applied Math-Computer Science. She's interested in the applications and social impacts of machine learning.",
			title: 'Deep Learning for ELVO Stroke Detection',
		},

		mona_jalal: {
			name: 'Mona Jalal',
			bio: 'Mona Jalal is a second year CS Ph.D. fellow in computer vision working with Professor Margrit Betke. She did a summer research internship at NVIDIA where she made a synthetic 6DOF object pose estimation dataset for deep learning applications using NVIDIA internal tools under direct guidance of Jonathan Tremblay, Thang To, and Josef Spjut. Prior to joining BU, she was an R&D Engineer 1 at University of California, Berkeley working on gesture recognition for augmented reality at Center for Augmented Cognition with Dr. Allen Yang. Prior to that she was a computer vision and machine learning intern at University of Wisconsin-Madison under supervision of Professor Vikas Singh working on object detection and creating synthesized dataset for semantic segmentation by playing video games. She obtained a double degree master’s in EE and CS from University of Wisconsin-Madison in 2016 and 2014 respectively. Her main interests are vision and language, domain adaptation, human pose estimation, gesture recognition, and facial analysis for affective computing.',
			title: 'Creating Synthetic Data for Deep Learning Applications',
			abstract: 'In this talk I will present some of the recent works done for creating synthetic datasets that could be combined with real datasets or used in stand-alone fashion for training deep neural network. While the focus of my talk will be creating synthetic data creation for object pose estimation for YCB kitchen items using a Plugin for Unreal Engine 4 by NVIDIA named NVIDIA Deep Learning Data Synthesizer and visualize the synthetic dataset 3d cuboids and 2d bounding box on the captured items, I will also go over creating dataset using AAA Video Games like Grand Theft Auto V using a Graphics Debugger named RenderDoc. At the end of session, I will showcase concept of Domain Randomization that deals with adding flying distractors, extreme lighting, and other various randomizations like changes in the object texture, rotation, movement and camera movement randomly. Use of Synthetic Datasets with Domain Randomization is a necessity in applications like 6DOF object pose estimation in which creating a ground truth is nearly impossible.  Other important applications include semantic segmentation, object detection and object tracking.',
		},

		olivia_koshy: {
			name: 'Olivia Koshy',
			bio: "Olivia is currently a senior at UC Berkeley studying Computer Science. She leads Machine Learning at Berkeley, a student org aimed at fostering an ML community at Berkeley, and also works on research with the Berkeley Law School's Human Rights Center on the intersection of AI and Children's rights. She's previously worked on Facebook ML and Probability teams as well as Skydio's Computer Vision team.",
			title: 'Your Children and AI',
			abstract: 'It is no longer a secret or a myth that the use of machine learning will have a direct impact on our lives. The next generation of children will be most affected by this technology, as they will be born and raised surrounded by big data collection and ML systems that will, amongst others, make decisions regarding their future in numerous capacities. Because of this exponential advancement of technology in the past years, the current international framework that protect children’s rights does not address most of the issues at stake with the development and usage of AI. The impact and intersection with human rights and particularly children’s rights is one of the most pressing issues that not just principal stakeholders, corporations and States, should be addressing, but also conversations those working with this technology first hand should be engaging in.',
		},

		sarah_bargal: {
			name: 'Sarah Bargal',
			bio: 'Sarah is a PhD candidate in the Image and Video Computing Group in the Department of Computer Science at Boston University. She is an IBM PhD fellow and a Hariri PhD Fellow. Her research interests lie in the intersection of Computer Vision and Machine Learning. Sarah is advised by Prof. Stan Sclaroff; They are particularly interested in developing deep learning formulations for the analysis of human motion and activities in video. Sarah received her MSc from the American University in Cairo, after which she became a lecturer of Computer Science at the Gulf University for Science and Technology.',
			title: 'Spatio-temporal Grounding in Visual Data',
			abstract: "Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.",
		},

		sicong_sheldon_huang: {
			name: 'Sicong (Sheldon) Huang',
			bio: 'Sheldon(Sicong) Huang finished his third year of undergrad at University of Toronto and is currently on a year of research internship at Vector Institute and Borealis AI, and after that he will return to school for his fourth year of undergraduate study. He joined Vector Institute to work with Professor Roger Grosse after his second year of undergrad and then joined Borealis AI as a research intern during his third year undergrad. His past work includes quantitative evaluation of generative models, CipherGAN (ICLR 2018) and musical style transfer (TimbreTron). He has a broad range of research interests including information theory, generative models, network compression, reinforcement learning and cognitive science. He is also the Founder and Former President of University of Toronto Machine Intelligence Student Team (UTMIST), where he currently serves as the Scientific Advisor.',
			title: 'TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer',
			abstract: 'In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, an audio processing pipeline which combines three powerful ideas from different domains: Constant Q Transform (CQT) spectrogram for audio representation, a variant of CycleGAN for timbre transfer and WaveNet-Synthesizer for high quality audio generation. We verified that CQT TimbreTron in principle and in practice is more suitable than its STFT counterpart, even though STFT is more commonly used for audio representation. Based on human perceptual evaluations, we confirmed that timbre was transferred recognizably while the musical content was preserved by TimbreTron.',
		},

		thalia_rossitter: {
			name: 'Thalia Rossitter',
			bio: 'Thalia is a recent graduate in Biostatistics and Economics from Simmons University. She is driven by intersection of information, engineering and philosophy that is Machine Intelligence. This summer she has been working with iD tech Coding Academy, teaching Machine Learning and Deep Neural Networks.',
			title: 'Performance Boosting Methods in Semi-Supervised NLP Neural Net Environments',
			abstract: 'My talk today will cover the use of AI/ML in natural language processing (NLP). The field of text analytics broadly refers to the component of artificial intelligence to understand human language, a topic made difficult by the inconsistencies in human communication. This talk with utilize a set of statistical techniques for identifying and grouping large text data sets, and how much machine models can be gained by using a preprocessed data which exhibits both semantic and syntactic similarity. This talk will be accessible without knowing the trends and research within the field today. It begins with a brief review of the rules and patterns and compares a logistic regression in percentages of accuracy to a reorganized model. I include some original research within vector-based recurrent nets and the leap in NLP performance that followed the representation of words as continuous vectors, which I will argue is the technique that will allow the field to grow the most effectively in the coming years.',
		},

		vitali_petsiuk: {
			name: 'Vitali Petsiuk',
			bio: 'Vitali Petsiuk is a 2nd-year Computer Science Ph.D. student advised by Professor Kate Saenko at Boston University. He does research in Computer Vision and Deep Learning, currently working on Explainable AI, aiming to develop tools for interpretable model analysis. Vitali got his BS and MS in Computer Science from Belarusian State University, where he worked, among other things, on Graph Theory and Image Segmentation.',
			title: 'RISE: Randomized Input Sampling for Explanation of Black-box Models',
			abstract: "Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.",
		},

		ellick_chan: {
			name: 'Ellick Chan (Intel)',
			bio: 'Ellick Chan is the Head of University Relations and Research at Intel AI. He earned a PhD in Computer Science from the University of Illinois at Urbana',
			title: 'Tensorflow optimizations and performance tuning for Intel platforms',
			abstract: 'Tensorflow is a popular software framework used for deep learning. Intel’s introduction of AVX-512 vectorized hardware greatly accelerates neural network inference and training at data center scale. This talk describes the software and hardware optimizations Intel has made to Tensorflow to leverage AVX-512 and how end users can leverage tools such as Tensorflow Timeline and VTune to optimize their AI workloads.',
		},

		adam_lesnikowski: {
			name: 'Adam Lesnikowski (Nvidia)',
			bio: 'Adam Lesnikowski is a Machine Learning Scientist at Nvidia.',
			title: 'Data-Driven Dataset Creation: Deep Active Learning for Autonomous Vehicles and Beyond',
			abstract: 'In this talk, the audience will learn about deep active learning and how it has been successfully applied to the autonomous vehicle project at Nvidia, as well as future research directions on learning from limited labels and optimal construction of datasets. We will cover projects that we have done on autonomous vehicle and other datasets to learn using limited labels, and outline a vision of the importance of advanced data driven methods to build datasets. Time permitting we will cover more recent exploratory experiments towards the optimal construction of training sets for today’s AI and deep neural network models.',
		},

		varun_jampani: {
			name: 'Varun Jampani (Nvidia)',
			bio: 'Varun Jampani is a Research Scientist at Nvidia.',
			title: 'Sparse High-Dimensional Neural Networks',
			abstract: 'The recent years have witnessed a dramatic increase in the adoption of convolutional neural networks (CNN) for a wide range of computer vision problems. ‘Convolutions’,  which form the building blocks of most CNN architectures, are designed to process non-sparse data that lie on 1D, 2D or 3D grid. In this work, we propose a generalization of convolution operation to process sparse and high-dimensional data (for instance data that lies in 5D or 6D space). Specifically, we make use of permutohedral lattices, traditionally used for bilateral filtering, and hash table for efficient processing of sparse high-dimensional data while being able to learn generic N-dimensional filters. The ability to learn generic high-dimensional filters allows us to stack several parallel and sequential filters like in a CNN resulting in a new breed of neural networks which we call ‘Bilateral Neural Networks’ (BNN). We demonstrate the use of BNNs on several 2D, video and 3D vision tasks. Experiments on diverse datasets and tasks demonstrate the use of BNNs for a range of vision problems.',
		},

		laurens_van_der_maaten: {
			name: 'Laurens van der Maaten (Facebook)',
			bio: 'Research Scientist: Facebook Artificial Intelligence Research Team (FAIR)',
		},

		lenny_grokop: {
			name: 'Lenny Grokop (Facebook)',
			bio: 'Software Engineer: Community Integrity',
		},

		ali_amir_aldan: {
			name: 'Ali-Amir Aldan',
			bio: 'I graduated MIT in 2017 majoring in course 6-3. And I currently work at an early stage startup Akruta in Palo Alto, CA. We are working with computer vision, high efficiency video compression and audio processing to bring a new way of communication in a home environment. This work has been an exciting part of my life for the past year. Before that I worked at Google as part of Video Search team.',
		},

		bristy_sikder: {
			name: 'Bristy Sikder',
			bio: 'I interested in the Machine Learning Systems and Security. I am currently an intern in DeepMind working with the Virtual Brain Analytics (VBA) team and Safe and Rigorous AI team to better understand and evaluate robustness in reinforcement learning in the domain of agent navigation. Prior to this, I have worked interned at Google Translate, Quora, Yugabyte and Bridgewater. In addition that I am also a medalist in International Olympiad in Informatics and I am passionate about encouraging women to pursue STEM.',
		},

		hassan_kane: {
			name: 'Hassan Kané',
			bio: 'Hassan Kané is part of the Class of 2017 at MIT where he studied Computer Science and Engineering with a minor in mathematics. His AI research experience spans NLP and includes work on clinical notes de-identification, medical concept embeddings (NIPS 2017 ML4HC workshop) as well as robotics R&D conducted as an intern in the radar perception group at Uber ATG. He also has a track record of combining visionary leadership and technical skills by notably co-founding then leading the machine intelligence community for 1.5 years. All these experiences help him in his current role as Chief Technology Officer (CTO) of Sela Labs, an 8 person startup working on using chatbots, satellite images, financial transactions to foster transparency and accountability in the international development sector. A fun fact is that he served as DJ for a NIPS 2017 party where he got Yann LeCun and Jeff Dean dancing to West African music.',
		},

		jeremy_nixon: {
			name: 'Jeremy Nixon',
			bio: 'Jeremy joined the MIC community in 2017, giving talks and presenting papers while he built a deep learning library on Apache Spark. Now he works on meta-learning and causal reinforcement learning at Google Brain.',
		},

		justin_chen: {
			name: 'Justin Chen',
			bio: 'I first learned the words "artificial intelligence" from my aunt when I was seven and have been dreaming of an AI future ever since. Today, I am the president and director of the Machine Intelligence Community, Inc., which is a non-profit dedicated to fostering the future researchers and engineers in machine intelligence by democratizing knowledge and building community.',
		},

		michael_chang: {
			name: 'Michael Chang',
			bio: 'Michael Chang is a 2nd year Ph.D. student at U.C. Berkeley advised by Sergey Levine and Tom Griffiths. At MIT he worked in Josh Tenenbaum’s lab on representation learning and intuitive physics. He has also interned with Jürgen Schmidhuber at IDSIA and Honglak Lee at the University of Michigan. He is currently interested in building learners that exploit compositionality in their internal representations as well as their internal computations in the hope that such compositionality would enable better extrapolation and knowledge transfer to future problems the learner encounters. Please see <a href="http://mbchang.github.io/">http://mbchang.github.io/</a> for more details.',
		},

		shaman_ray_chaudhuri: {
			name: 'Shraman Ray Chaudhuri',
			bio: 'Shraman graduated from MIT in 2018 and now works on meta-optimization methods at Google as an AI Resident. While at MIT, he worked with Prof. Nir Shavit on computational connectomics and Prof. Josh Tenenbaum on simulator-based reasoning. Outside of work, Shraman enjoys hiking, basketball, and heated rounds of Secret Hitler. Feel free to reach out if you have questions about research / Google / anything else!',
		},

		simanta_gautam: {
			name: 'Simanta Gautam',
			bio: 'Simanta is a co-founder of Synapse Technology, a computer vision company that secures millions of people around the world with automated threat detection at x-ray checkpoints. A computer science major at MIT, Simanta previously worked on reinforcement learning, computational cognitive science, and vision. His most treasured memories from MIT are of social interactions with fellow students, many of whom are in the alumni panel. Outside of work, Simanta is interested in subsistence farming, space, and irony.',
		},

	}
};
